# Regression Model Quiz

This quiz is part of Algoritma Academy assessment process. Congratulations on completing the Regression Model course! We will conduct an assessment quiz to test practical regression model techniques you have learned on the course. The quiz is expected to be taken in the classroom, please contact our team of instructors if you missed the chance to take it in class.

## Data Exploration

In this quiz, you will be using criminologist dataset (`crime`). If you have been following along in the course you can use the one that has already been loaded in the environment or you can run the following chunk in your RMarkdown to make sure we are using a same column names:

```{r}
crime <- read.csv("crime.csv")
colnames(crime) <- c("percent_m", "is_south", "mean_education", "police_exp60", "police_exp59", "labour_participation", "m_per1000f", "state_pop", "nonwhites_per1000", "unemploy_m24", "unemploy_m39", "gdp", "inequality", "prob_prison", "time_prison", "crime_rate")
```

To make sure you have loaded the data correctly, do a simple inspection of the data. Try to peek in using `head` or `tail` and see if the columns have been stored in its appropriate data types.
```{r}
# your code here
head(crime)
str(crime)
```

Among all variables within our `crime` dataset, there is a `crime_rate` variable that describes the measure of crime rate of the United States in 1960 from different states. Imagine you're working as a government analyst and would like to see how social-economic conditions could reflect on the crime rate in a state. Recall how we can inspect the correlation for each variable using `cor` or `ggcorr` from `GGally` package. Try it out on your own and see what are the possible predictor variables for our `crime_rate` variable.

```{r}
# your code here
library(GGally)

ggcorr(crime, label = T)
```

Based on the result of the previous chunk, you should know each variable's correlation with `crime_rate` variable. Referring to the value, we can perform a preliminary variable selection for a suitable predictor variable.
___
1. Which variable has little to no correlation with our `crime_rate` variable and might not be suitable as a predictor variable?
  - [ ] crime_rate
  - [ ] police_exp59
  - [ ] unemploy_m39
  - [x] nonwhites_per1000
___

## Building Linear Regression    

From the data exploration process and the correlation information of our dataset, it is known that not all variables show a strong correlation with `crime_rate` variable. Let's try to build a simple linear model using one of the highly correlated variables: `police_exp59`. Create a regression model using `lm()` function to predict `crime_rate` using `police_exp59` from our dataset and assign it to an object named `model_crime`.

```{r}
# your code here
crime_model <-  lm(crime_rate ~ police_exp59, crime)

crime_model
```
```{r}
summary(crime_model)
```

___
2. Which of the following best describes the slope?
  - [ ] It's a negative slope, and is statistically insignificant (P-value higher than 0.05)
  - [x] It's a positive slope, and is statistically significant (P-value lower than 0.05)
  - [ ] It's a positive slope, and is statistically insignificant (P-value higher than 0.05)
  - [ ] It's a negative slope, and is statistically significant (P-value lower than 0.05)

3. What is the most fitting conclusion from the regression model above?
  - [ ] The R-squared does not tell us about the quality of our model fit, we should use p-value instead
  - [ ] The R-squared approximates 0.44, indicating a reasonable fit (the closer to 0 the better)
  - [x] The R-squared approximates 0.44, indicating a poor fit (the closer to 1 the better)
___

## Feature Selection using Stepwise Regression

The R-squared of `model_crime` approximates 0.44, which ideally needs to be improved upon by, for example, adding more predictor variables. One of the techniques for selecting predictor variables is using stepwise regression algorithm. Use the `step()` function with `direction="backward"` parameter and store the best model estimate under the `model_step` object.

```{r}
# your code here
full_model <- lm(crime_rate ~ . , crime)

model_step <- step(full_model, direction = "backward")
```
___

```{r}
summary(model_step)
```

4. Based on the summary of your final model, which statement is **incorrect**?
  - [ ] An increase of 1 of police_exp60 causes the value of crime_rate to increase by 10,265
  - [x] An increase of 1 of unemploy_m24 causes the crime_rate to decrease by 6,087
  - [ ] An increase of 1 of mean_education causes the value of crime_rate to decrease by 18.01
  - [ ] Adjusted R-squared is a better metrics for evaluating our model compared to Multiple R-squared
___

## Shapiro test for Normality test

One of the assumptions for linear regression stated that the error obtained from the model must be distributed normally around the mean of 0. You will need to validate our normality assumption from `model_step` using `shapiro.test()` function. This function requires us to pass in the residuals of our model.

```{r}
hist(model_step$residuals)
```


```{r}
# your code here
shapiro.test(model_step$residuals)
```

For your reference, Shapiro testing use the following hypotheses:

$H_0$ : Error is distributed normally  

$H_1$ : Error is not distributed normally  

___
5. Based on the Shapiro test you have performed, what conclusion can be drawn from the result?
  - [x] Error is distributed normally (P-value higher than 0.05) 
  - [ ] Error is distributed normally (P-value lower than 0.05) 
  - [ ] Error is not distributed normally (P-value higher than 0.05) 
  - [ ] Error is not distributed normally (P-value lower than 0.05) 
___

## Breusch-Pagan for Heteroskedasticity Test

Another assumption you need to test is whether or not the error of our model is homoscedastic. Homoscedastic means the error is distributed with equal variance over different data ranges. To test this behavior, you can use the `bptest` function from `lmtest` package and pass in our model.

For your reference, Breusch-Pagan testing use the following hypotheses:

$H_0$: Error is considered Homoscedastic  

$H_1$: Error is considered Heteroscedastic  

```{r}
# your code here
library(lmtest)
bptest(model_step)
```
___
6. Based on Breusch-Pagan test you have performed, what conclusion can be drawn from the result?
  - [x] Heteroscedasticity is not present
  - [ ] Heteroscedasticity is present
  - [ ] The data spreads normally
  - [ ] There is no correlation between residuals and target variable
___

## Variance Inflation Factor

Using VIF value, we can determine whether or not there are multicollinearity between predictor variables. A high VIF value indicates a high correlation between the variables. You can use the `vif` function from `car` package from our model. Pass in our `model_step` object into the function and see if there's a multicollinearity in the model.

```{r}
# your code here
library(car)

vif(model_step)
```
___
7. Based on the VIF value, which interpretation is correct?
  - [ ] inequality does not significantly affect crime_rate
  - [ ] An increase of 1 value on mean_education causes the value of crime_rate to increase by 4.1
  - [x] Multicollinearity is not present in our model because the VIF values for all variables are below 10 
  - [ ] Variables with multicollinearity should not be removed from model
  ___

## Predicting Unseen Data

Along this quiz session, you have performed statistical tests to make sure the model passed the assumptions of a linear regression model. Now imagine you were given a new dataset that records the same socio-economic variables from different observations. The data is stored under `crime_test.csv`, please read in the data and store it under an object named `crime_test`. Next, create a prediction using the new data using our `model_step` object. You can store the prediction values under a new column in the `crime_test` data frame.

```{r}
# your code here
crime_test <- read.csv("crime_test.csv")
```

```{r}
prediction <- predict(model_step, newdata =crime_test)

head(prediction)
```

Now pay attention to the new dataset provided. Among the variables you should see a `crime_rate` column describing the real crime rate value of the observations. Within the workshop you have learned some metrics to evaluate our model performance. Try to calculate the `MSE` of our `model_step` prediction. You can use the `MSE` function from `MLMetrics` package by passing in `y_true` and `y_pred` parameter.

```{r}
# your code here
library(MLmetrics)
MSE(prediction, crime_test$crime_rate)
```
___
8. What is the MSE value of the crime_test prediction result? (round to two decimal points)    
  - [ ] 55027.7
  - [x] 46447.42
  - [ ] 45269.15
___
